---
title: "Statistical Models used for Smoothing and Scaling"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistical Models used for Smoothing and Scaling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

The **smoothscale** package provides functions for estimating counts and prevalences in multiple small populations. The outcomes of interest include attributes such as being below a poverty line, or being engaged in child labour.

Attempts to estimate counts and prevalences in small populations often have to content with two types of errors: sampling errors and measurement errors. Statisticans have developed many methods for dealing with these type of errors, under the heading of "small area estimation" [@rao2015small,@eclac2022child] and measurement error models REFERENCE. Many of these methods are complex, and require specialist statistical skills to implement and interpret. The methods implemented in **smoothscale** are, in contrast, deliberately simple.

This vignette defines sampling error and measurement error, and describes how they are dealt with by functions `smooth_prob()`, and `scale_prob()`, including a description of the statistical models that they implement. The vignette then discusses two further issues: stratification by additional variables such as age and sex/gender, and interactions between sampling and measurement errors.

# Sampling Errors

## The setting

We would like to estimate the prevalence of some attribute, such as employment or school attendance, in each of $k = 1, \cdots, K$ small areas. We are interested in the absolute number of people with the attribute in each area and in how the underlying propensity to have the attribute varies across areas. 

We have data on $n_k$ people from each area. A subset $x_k$ of people in each area have the attribute in question. Data on school attendance, for instance, might look like this:

| District | Respondents  | Respondents attending school |
|:---------|:------------:|:----------------------------:|
|     1001 |           83 |                           62 |
|     1002 |           25 |                           18 |
| $\vdots$ |     $\vdots$ |                     $\vdots$ |
|     1156 |           54 |                           41 |

If the data come from a census or administrative system, then the number of repondents may be approximately equal to size of the target population in each area. The number of children captured in the census, for instance, should be close to the actual number of children living in an area.

If, however, the data come from a survey, then the number of respondents is likely to be much smaller than the number of people in the target population. In this case, to estimate counts of people with the attribute in each area, we need an additional set of data, $N_k$, $k = 1, \cdots, K$, on the size of the target population in each area.  Data on a target population of school-age children, for instance, might look like this:

| District | Total school-age children  |
|:---------|:--------------------------:|
|     1001 |                       4027 |
|     1002 |                       1382 |
| $\vdots$ |                   $\vdots$ |  
|     1156 |                       2994 |


## The statistical challenge

All of the attributes that we might be interested in are, to some extent, affected by random variation. Whether a particular child is involved in child labour, for instance, depends partly on random factors such as the economic status of the child's family, and the accessibility of the local school. We can therefore thing of the number of people with the attribute has having two components:

1. an underlying, `expected' value, and
2. random error.

Similarly, the proportion of people in an area who have an attribute can be thought of as having two components:

1. an underlying propensity to have the attribute, and
2. random error.

The idea 'underlying propensity' is abstract, but nevertheless useful. Consider, for instance, a population of 10 people in which no one dies during the year. The observed proportion of people dying is zero. However, we would not think that the underlying propensity -- the true risk of dying -- was also zero.

In large populations, random errors tend to cancel out, so that observed numbers and proportions can be reliable indicators of underlying propensities. In small populations, there is much less scope for errors cancelling, so that observed numbers and proportions are much more noisy indicators.

Consider, for instance, two hypothetical areas:

| Area | Children      | Children engaged in child labour |
|:-----|:-------------:|:--------------------------------:|
| A    | 500           | 100                              |     
| B    | 5             | 1                                |

The `direct' estimate of the probability of having an attribute is the number of people with the attribute divided by the number of people, $x_k / n_k$. The direct estimate for Area A is the same as the direct estiamte for Area B, at 0.2. But we would be far less confident in the accuracy of the estimates for Area B than we would be in the estimate for Area B, since it is much easier for random variation to pull the observed proportion away from the true underlying probability in a small population than in a large population.

In many applications, we would like to strip away the random variation, and get at underlying expected values and propensities. In other words, we would like to replace the direct estimate $x_k / n_k$ an estimate that is less subject to random variation.

## Statistical model

Function `smooth_prob()` produces smoothed estimates for that try to compensate for the effects of random variation. The model draws on ideas from Bayesian statistics [@gelman2014bayesian], but in contrast to a fully Bayesian analysis, `smooth_prob()` only yields point estimates.

The estimates are based on a statistical model. The number of people in area $k$ with attribute of interest is treated as a random draw from a binomial distribution,
\begin{equation}
  x_k \sim \text{Binomial}(n_k, \pi_k),
\end{equation}
where $\pi_k$ is the probability of having the attribute. The goal is to estimate $\pi_k$ for all $K$ areas.

The $\pi_k$ are treated as coming from a beta distribution,
\begin{equation}
  \pi_k \sim \text{Beta}(\alpha, \beta).
\end{equation}
The values of $\pi_k$ are assumed to be distributed around a central value. The location of that central value, and the tightness of the distribution depends on the values taken by parameters $\alpha$ and $\beta$, which are estimated from the data.

When estimating $\alpha$ and $\beta$ it is convenient to first transform them into new parameters with clearer interprentations,
\begin{align}
  \alpha & = \lambda \nu \\
  \beta & = (1 - \lambda) \nu
\end{align}
Parameters $\lambda$ is the expected value for $\pi_k$, and parameter $\nu$ how tightly the $\pi_k$ are clustered around $\lambda$ (with larger values implying tighter clustering). No constraints are placed on $\lambda$, except that is restricted to the range
$0 < \lambda < 1$. The $\nu$ parameter is assumed to come from a log-Normal distribution,
\begin{equation}
  \nu \sim \text{LogNormal}^+(M, 1),
\end{equation}
with $M$ set at at a value of $\log 10$. The distributional assumptions about $\nu$ have virtually no impact on estimates except when the number of small areas and the population of each area are smll, in which case they help stabilise the estimates.

Fitted values for $\lambda$ and $\nu$ can be obtained by finding the values that maximise the quantity quantity
\begin{equation}
  \prod_{k=1}^K \text{BetaBinom}(x_k | n_k, \lambda \nu, (1 - \lambda) \nu) \text{LogNormal}(\nu | \log 10, 1).
\end{equation}
(This quantity is proportional to the posterior distribution, or, in non-Bayesian terms, to the penalised likelihood. See @enwiki:1184063616 on the definition of the beta-binomial distribution.) Values for the $\pi_k$ can then be derived using the proporties of the binomial and beta distributions [@enwiki:1167875362]. The fitted value for $\pi_k$ is
\begin{equation}
  \hat{\pi}_k = \phi_k \frac{x_k}{n_k} + (1 - \phi_k) \hat{\lambda}
\end{equation}
where
\begin{equation}
  \phi_k = \frac{n_k}{n_k + \hat{\nu}_k}.
\end{equation}

A smoothed value for the count of people with the attribute is then $\hat{\pi}_k N_k$ (which equals $\hat{\pi}_ n_k$ in cases where $n_k$ comes from a complete enumeration of the population.)


## Partial pooling

The fitted value $\hat{\pi}_k$ is a weighted average of the direct estimate for area $k$, $\frac{x_k}{n_k}$, and central value $\hat{\lambda}$. The weights depend on sample size $n_k$. The higher $n_k$ is, the closer $w_k$ is to 1, and the closer $\hat{\pi}$ is to direct $\frac{x_k}{n_k}$. Conversely, values of $\hat{\pi}_k$ where $n_k$ is small will lie close to $\hat{\lambda}$. REFER TO FIRST VIGNETTE.

WHY IS THIS A GOOD THING.

# Measurement error

## Goal

The area-level tabulations may also be affected by measurement error. For instance, some respondents may have misinterpreted the questions, or the questions may used different definitions from the ones that we need. 


- as above

## Data

As above, plus national-level

In some cases, reliable national-level data on prevalences might also be available. There might, for instance, be reliable national-level estimate of the proportion of children attending school. A common source of reliable national-level data is household surveys. Household surveys typically do not have large enough samples to provide stable estimates for small areas. However, they typically do have large enough samples to provide stable estimates at the national level, while also having relatively low measurement errors.

The reliable national-level prevalences may be disaggregated by subpopulation. For instance, national-level school attendance data might look like this:

|   Age | Sex    | Percent of children attending school |
|-------|--------|:------------------------------------:|
|   5-9 | Female |                                   92 |
|   5-9 | Male   |                                   91 |
| 10-14 | Female |                                   84 |
| 10-14 | Male   |                                   82 |




## Statistical challenges

- No sampling model
- Have idea of average size of adjustment, but not particular

## Model

Let $\pi_k^*$ denote a scaled version of $\pi_k$. We know the national total
\begin{equation}
  \bar{\pi}^* = \sum_{k=1}^K w_k \pi_k^*
\end{equation}
and would to derive the individual $\pi_k$. 

Assume
\begin{equation}
  \pi_k^* =
    \begin{cases}
      \pi_k + \rho (1 - \pi_k) & \text{ if} \pi_k < \bar{\pi}^* \\
      \pi_k - \rho \pi_k & \text{ otherwise}.
    \end{cases}
\end{equation}

\begin{equation}
  \bar{\pi}^* = \sum_{k=1}^K w_k (\pi_k + \rho (1 - \pi_k)) = \bar{\pi} + \rho (1 - \bar{\pi})
\end{equation}

\begin{equation}
  \rho = \frac{\bar{\pi}^* - \bar{\pi}}{1 - \bar{\pi}}
\end{equation}



# Stratification

Often the data is also disaggregated by subpopulation. For instance, data disaggregated by age and sex might look like this:

| District |      Age | Sex      | All Children | Children attending school |
|----------|----------|----------|:------------:|:-------------------------:|
|     1001 |      5-9 | Female   |           14 |                        10 |
|     1001 |      5-9 | Male     |           16 |                        13 |
|     1001 |    10-14 | Female   |           28 |                        21 |
|     1001 |    10-14 | Male     |           25 |                        18 |
| $\vdots$ | $\vdots$ | $\vdots$ |     $\vdots$ |                  $\vdots$ |
|     1156 |      5-9 | Female   |           11 |                        15 |
|     1156 |      5-9 | Male     |           13 |                        13 |
|     1156 |    10-14 | Female   |           17 |                         5 |
|     1156 |    10-14 | Male     |           13 |                         8 |


# Combining

