[{"path":"https://bayesiandemography.github.io/smoothscale/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 International Labour Organization Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Statistical Models used for Smoothing and Scaling","text":"smoothscale package provides functions estimating counts prevalences small populations. functions deal two problems often arise sort estimation: sampling errors measurement errors. Statisticans developed many methods estimation small populations, headings “small area estimation” (Rao Molina 2015). Many methods complex, require specialist statistical skills. methods used smoothscale , contrast, deliberately simple. vignette defines sampling measurement errors, describes dealt functions smoothscale. vignette discusses two issues: stratification additional variables age sex/gender, interactions sampling measurement errors.","code":""},{"path":[]},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"the-setting","dir":"Articles","previous_headings":"Sampling Errors","what":"The setting","title":"Statistical Models used for Smoothing and Scaling","text":"like estimate prevalence attribute, employment school attendance, \\(k = 1, \\dots, K\\) small areas. interested absolute numbers underlying probabilities. data \\(n_k\\) people area. number people area reported attribute question \\(x_k\\). following hypothetical data, instance, \\(n_k\\) number respondents distribution, \\(x_k\\) number respondents attend school: data come census administrative system, number repondents may approximately equal size target population area. number children captured census, instance, close actual number children living area. , however, data come sample extracted census, number respondents likely much smaller number people target population. case, estimate counts people attribute area, need additional set data, \\(N_k\\), \\(k = 1, \\cdots, K\\), size target population area. Data target population school-age children, instance, might look like : Observed prevalences always, extent, reflect random variation. Whether particular child involved child labour, instance, depends partly random factors economic status child’s family. data come survey, sample selection procedures add randomness. can therefore think number people attribute reflecting stable ‘expected’ value, random error. Similarly, proportion people area attribute reflects underlying propensity attribute, random error. small populations, can essential distinguish observed proportion underlyign propensity. Consider, instance, population 10 people , given yer, one dies. observed proportion people dying year zero. However underlying propensity – risk dying people fact faced year – zero. large populations, random errors tend cancel , observed counts can reliable indicators underlying propensities. small populations, much less scope cancellation, observed counts much noisy. statistical challenge minimise effects noise, get underlying expected values propensities.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"statistical-model","dir":"Articles","previous_headings":"Sampling Errors","what":"Statistical model","title":"Statistical Models used for Smoothing and Scaling","text":"Function smooth_prob() produces smoothed estimates try strip effects random variation. model draws ideas Bayesian statistics (Gelman et al. 2014), contrast fully Bayesian analysis, smooth_prob() yields point estimates. estimates based statistical model. number people area \\(k\\) attribute interest treated random draw binomial distribution, \\[\\begin{equation}   x_k \\sim \\text{Binomial}(n_k, \\pi_k), \\end{equation}\\] \\(\\pi_k\\) probability attribute. job smooth_prob() estimate \\(\\pi_k\\) \\(K\\) areas. Function smooth_prob() assumes \\(\\pi_k\\) come beta distribution, \\[\\begin{equation}   \\pi_k \\sim \\text{Beta}(\\alpha, \\beta). \\end{equation}\\] centre distribution, extent values concentrate around centre, determined parameters \\(\\alpha\\) \\(\\beta\\). Values parameters estimated data. Rather work directly \\(\\alpha\\) \\(\\beta\\), express terms two new parameters, \\[\\begin{align}   \\alpha & = \\lambda \\nu \\\\   \\beta & = (1 - \\lambda) \\nu \\end{align}\\] Parameter \\(\\lambda\\) central value \\(\\pi_k\\), parameter \\(\\nu\\) governs tightly \\(\\pi_k\\) concentrated around \\(\\lambda\\), larger values implying greater concentration. place constraints \\(\\lambda\\), except restrict range \\(0 < \\lambda < 1\\). assume \\(\\nu\\) parameter comes log-normal distribution, \\[\\begin{equation}   \\nu \\sim \\text{LogNormal}(\\log 10, 1). \\end{equation}\\] distributional assumptions \\(\\nu\\) virtually impact estimates \\(\\pi_k\\), except number small areas population area small. Placing soft constraint \\(\\nu\\) , however, help stabilise estimates reasonable values counts small. Fitted values \\(\\lambda\\) \\(\\nu\\) can obtained finding values maximise quantity \\[\\begin{equation}   \\prod_{k=1}^K \\text{BetaBinom}(x_k | n_k, \\lambda \\nu, (1 - \\lambda) \\nu) \\text{LogNormal}(\\nu | \\log 10, 1). \\end{equation}\\] quantity proportional posterior distribution, , non-Bayesian terms, penalised likelihood. See Wikipedia contributors (2023a) definition beta-binomial distribution. Values \\(\\pi_k\\) can derived using properties binomial beta distributions (Wikipedia contributors 2023b). fitted value \\(\\pi_k\\) \\[\\begin{equation}   \\hat{\\pi}_k = \\frac{x_k + \\lambda \\nu}{n_k + \\nu} (\\#eq:pihat) \\end{equation}\\] smoothed value number people attribute \\(\\hat{\\pi}_k N_k\\), , data complete enumeration population, \\(\\hat{\\pi}_k n_k\\).","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"partial-pooling","dir":"Articles","previous_headings":"Sampling Errors","what":"Partial pooling","title":"Statistical Models used for Smoothing and Scaling","text":"formula \\(\\hat{\\pi}_k\\) given @ref(eq:pihat) can rewritten \\[\\begin{equation}   \\hat{\\pi}_k = \\phi_k \\frac{x_k}{n_k} + (1 - \\phi_k) \\hat{\\lambda}     (\\#eq:partial) \\end{equation}\\] \\[\\begin{equation}   \\phi_k = \\frac{n_k}{n_k + \\hat{\\nu}_k}. \\end{equation}\\] quantity \\(\\frac{x_k}{n_k}\\) @ref(eq:partial) traditional ‘direct’ estimate probability area \\(k\\). direct estimate based entirely data area \\(k\\), smoothing. parameter \\(\\hat{\\lambda}\\) estimate average value across areas, uses available data. parameter \\(\\phi_k\\) type weight, close 1 \\(n_k\\) large, close 0 \\(n_k\\) small. Equation @ref(eq:partial) shows \\(\\hat{\\pi}_k\\) can regarded compromise local estimate \\(\\frac{x_k}{n_k}\\) global estimate \\(\\hat{\\lambda}\\). way weights \\(\\phi_k\\) constructed means local estimate exerts greater influence data support , less influence less data support . data-driven balancing local global estimates common small area estimation, known, among things, “partial pooling”. Partial pooling makes intuitive sense, usually performs well practice. characteristic partially-pooled estimates tightly concentrated direct estimates. form smoothing, also referred shrinkage regularisation. estimator @ref(eq:partial) property. Partial pooling shifts \\(\\pi_k\\) towards \\(\\hat{\\lambda}\\), concentrated direct estimates \\(\\frac{x_k}{n_k}\\).","code":""},{"path":[]},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"setting","dir":"Articles","previous_headings":"Measurement error","what":"Setting","title":"Statistical Models used for Smoothing and Scaling","text":"second sort error often affects small area estimates measurement error. Measurement error can occur, instance, respondents misunderstand question, proxy respondents used, definitions used data different ones required analysis. Accounting measurement errors normally requires sort additional data true underlying values. Function scale_prob(), function smoothscale deals measurement error, requires reliable data national-level prevalences. typical source reliable national-level data household surveys. Although household surveys large enough samples provide stable estimates small areas, typically large enough samples provide stable estimates national level, also relatively low measurement errors.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"model","dir":"Articles","previous_headings":"Measurement error","what":"Model","title":"Statistical Models used for Smoothing and Scaling","text":"Let \\(p_k\\) initial estimate probability person area \\(k\\) possesses attribute question. think \\(p_k\\) differs true value \\(\\pi_k\\) measurement error, , \\[\\begin{equation}   \\pi_k = p_k + \\epsilon_k, \\end{equation}\\] \\(\\epsilon_k\\) area-level measurement error. estimate \\(\\pi_k\\), need way estimating \\(\\text{epsilon}_k\\). Let \\[\\begin{equation}   w_k = \\frac{N_k}{\\sum_{k=1}^K N_k} \\quad \\text{} \\quad \\frac{n_k}{\\sum_{k=1}^K n_k}, \\quad k = 1, \\cdots, K, \\end{equation}\\] known set population weights. Let \\[\\begin{equation}   \\bar{p} = \\sum_{k=1}^K w_k p_k \\end{equation}\\] national-level probability obtained averaging initial area-level estimates, let \\[\\begin{equation}   \\bar{\\pi} = \\sum_{k=1}^K w_k \\pi_k \\end{equation}\\] true national-level probability. can obtain accurate estimate \\(\\hat{\\pi}\\) , instance, national-level survey, though know individual \\(\\pi_k\\). national estimate, can calculate average error, \\[\\begin{equation}   \\bar{\\epsilon} = \\sum_{k=1}^K w_k \\epsilon_k = \\sum_{k=1}^K w_k (\\pi_k - p_k) = \\sum_{k=1}^K w_k \\pi_k - \\sum_{k=1}^K w_k p_k = \\bar{\\pi} - \\bar{p}. (\\#eq:epsilon) \\end{equation}\\] like construct estimator \\(\\epsilon_k\\) form \\[\\begin{equation}   \\epsilon_k = \\alpha_k \\bar{\\epsilon} \\end{equation}\\] \\(\\alpha_k\\) area-specific scaling factor. estimator implemented scale_prob() takes slightly different forms average error positive (need scale upwards) average error negative (need scale downwards): \\[\\begin{equation}   \\alpha_k = \\begin{cases}     \\displaystyle \\frac{1 - p_k}{1 - \\bar{p}}  & \\text{} \\bar{\\epsilon} > 0 \\\\     \\displaystyle \\frac{p_k}{\\bar{p}}  & \\text{} \\bar{\\epsilon} < 0.     \\end{cases} \\end{equation}\\] scaling factor \\(\\alpha_k\\) irrelevant \\(\\bar{\\epsilon} = 0\\), case scaling necessary. national-level error \\(\\bar{\\epsilon}\\) positive, \\(\\alpha_k\\) non-negative, \\(\\pi_k\\) always greater equal \\(p_k\\). Conversely, \\(\\bar{\\epsilon}\\) negative, \\(\\alpha_k\\) non-positive, \\(\\pi_k\\) always less equal \\(p_k\\). scale_prob() estimator \\(\\pi_k\\) passes basic sanity checks: adjusting upwards, \\(\\pi_k\\) never exceeds 1, adjusting downwards, \\(\\pi_k\\) less never less 0, adjusting either direction, \\(\\epsilon_k\\) always sum \\(\\bar{\\epsilon}\\) defined @ref(eq:epsilon). (See Appendix details.) estimator also smooths estimates, relative original \\(\\pi_k\\), pulling towards central value. adjusting upwards, size adjustment proportional \\(1 - p_k\\), implying unusually low values get large adjustments, unusually high values get small adjustments. adjusting downwards, size adjustment proportional \\(p_k\\), implying unusually high values get large adjustments, unusually low values get small adjustments. adjustment process pulls extreme values towards centre, reminiscent shrinkage occurs partial pooling. assume extreme symptom measurement error, smoothing behaviour , average, appropriate.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"stratification","dir":"Articles","previous_headings":"","what":"Stratification","title":"Statistical Models used for Smoothing and Scaling","text":"Often data also disaggregated subpopulation. instance, data disaggregated age sex might look like : national-level prevalences required scale_prob() might like : smoothscale takes simple approach disaggregated: estimates constructed completely independently combination stratifying variables. instance, data , estimates constructed separately 5-9 year old females, 5-9 year old males, 10-14 year old females, 10-14 year old males. Although might gains accuracy stability sharing information across combinations variables, mean sacrificing simplicity usability.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"combinations-of-sampling-and-measurement-error","dir":"Articles","previous_headings":"","what":"Combinations of sampling and measurement error","title":"Statistical Models used for Smoothing and Scaling","text":"derivation estimator used smooth_prob() refers sampling error, derivation estimator used scale_prob() refers measurement error. data subject substantial sampling error measurement error? principle, might advantages modelling sampling error measurement error simultaneously. However, require extra assumptions nature measurement errors, ways interacted sample size. recommended approach smoothscale instead use smooth_prob() deal sampling error, use scale_prob() deal measurement error. approach much simpler, given difficulty formulating assuptions measurement errors, probably just accurate.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/articles/smoothscale-models.html","id":"appendix-sanity-checks-for-scale_prob-estimator","dir":"Articles","previous_headings":"","what":"Appendix: Sanity checks for scale_prob() estimator","title":"Statistical Models used for Smoothing and Scaling","text":"adjusting upwards, \\(\\pi_k\\) never greater 1, since \\[\\begin{equation}   \\pi_k = p_k + \\frac{1 - p_k}{1 - \\bar{p}} (\\bar{\\pi} - \\bar{p}) \\le p_k + \\frac{1 - p_k}{1 - \\bar{p}} (1 - \\bar{p}) = p_k + 1 - p_k = 1. \\end{equation}\\] Similarly, adjusting dowwards, \\(\\pi_k\\) less never 0, since \\[\\begin{equation}   \\pi_k = p_k + \\frac{p_k}{\\bar{p}} (\\bar{\\pi} - \\bar{p}) \\ge p_k + \\frac{p_k}{\\bar{p}} (-\\bar{p}) = p_k - p_k = 0. \\end{equation}\\] addition, adusting upwards, \\[\\begin{equation}    \\sum_{k=1}^K w_k \\epsilon_k = \\sum_{k=1}^K w_k \\frac{1 - p_k}{1 - \\bar{p}} \\bar{\\epsilon} = \\frac{1 - \\bar{p}}{1 - \\bar{p}} \\bar{\\epsilon} = \\bar{\\epsilon}, \\end{equation}\\] , adusting downwards, \\[\\begin{equation}    \\sum_{k=1}^K w_k \\epsilon_k = \\sum_{k=1}^K w_k \\frac{p_k}{\\bar{p}} \\bar{\\epsilon} = \\frac{\\bar{p}}{\\bar{p}} \\bar{\\epsilon} = \\bar{\\epsilon}, \\end{equation}\\] required.","code":""},{"path":[]},{"path":"https://bayesiandemography.github.io/smoothscale/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"John Bryant. Maintainer.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bryant J (2023). smoothscale: Smooth Counts Scale Benchmarks. R package version 0.1.0, https://bayesiandemography.github.io/smoothscale/.","code":"@Manual{,   title = {smoothscale: Smooth Counts and Scale to Benchmarks},   author = {John Bryant},   year = {2023},   note = {R package version 0.1.0},   url = {https://bayesiandemography.github.io/smoothscale/}, }"},{"path":"https://bayesiandemography.github.io/smoothscale/index.html","id":"smoothscale","dir":"","previous_headings":"","what":"Smooth Counts and Scale to Benchmarks","title":"Smooth Counts and Scale to Benchmarks","text":"Simple small area estimation methods.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Smooth Counts and Scale to Benchmarks","text":"","code":"devtools::install_github(\"bayesiandemography/smoothscale\")"},{"path":"https://bayesiandemography.github.io/smoothscale/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Smooth Counts and Scale to Benchmarks","text":"","code":"library(smoothscale) library(dplyr, warn.conflicts = FALSE) syn_census %>%   inner_join(syn_survey, by = c(\"age\", \"sex\")) %>%   group_by(age, sex) %>%   mutate(child_labour_sm = smooth_prob(x = child_labour,                                        size = all_children)) #> # A tibble: 200 × 8 #> # Groups:   age, sex [4] #>    area    age   sex    child_labour all_children total_child_labour #>    <chr>   <chr> <chr>         <int>        <dbl>              <dbl> #>  1 Area 01 5-9   Female           42          368             222365 #>  2 Area 02 5-9   Female           10           33             222365 #>  3 Area 03 5-9   Female          112          453             222365 #>  4 Area 04 5-9   Female          151          354             222365 #>  5 Area 05 5-9   Female           23          101             222365 #>  6 Area 06 5-9   Female            3            5             222365 #>  7 Area 07 5-9   Female            3           14             222365 #>  8 Area 08 5-9   Female            6           35             222365 #>  9 Area 09 5-9   Female           14           48             222365 #> 10 Area 10 5-9   Female          422         1889             222365 #> # ℹ 190 more rows #> # ℹ 2 more variables: total_all_children <dbl>, child_labour_sm <dbl>"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/estimate_alpha_beta.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate Parameters Used in Smoothing — estimate_alpha_beta","title":"Estimate Parameters Used in Smoothing — estimate_alpha_beta","text":"Estimate alpha beta parameters x size, maximising posterior density. internal function, normally called directly end users.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/estimate_alpha_beta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate Parameters Used in Smoothing — estimate_alpha_beta","text":"","code":"estimate_alpha_beta(x, size, prior_cases)"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/estimate_alpha_beta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate Parameters Used in Smoothing — estimate_alpha_beta","text":"x vector non-negative values. size vector non-negative values prior_cases non-negative scalar.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/estimate_alpha_beta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate Parameters Used in Smoothing — estimate_alpha_beta","text":"named numeric vector length 2.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/estimate_alpha_beta.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate Parameters Used in Smoothing — estimate_alpha_beta","text":"posterior density formed multiplying beta-binomial likelihood half-Cauchy prior sum alpha beta.","code":""},{"path":[]},{"path":"https://bayesiandemography.github.io/smoothscale/reference/estimate_alpha_beta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate Parameters Used in Smoothing — estimate_alpha_beta","text":"","code":"estimate_alpha_beta(x = syn_census$child_labour,                     size = syn_census$all_children,                     prior_cases = 100) #>    alpha     beta  #> 4.058873 9.697995"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/scale_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale Probabilities — scale_prob","title":"Scale Probabilities — scale_prob","text":"Scale probabilities consistent pre-specified benchmarks, totals obtained national survey.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/scale_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale Probabilities — scale_prob","text":"","code":"scale_prob(x, size, prob_target)"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/scale_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale Probabilities — scale_prob","text":"x Number successes area population. numeric vector. size Number trials area population. numeric vector. prob_target Benchmark probability. number 0 1.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/scale_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale Probabilities — scale_prob","text":"numeric vector scaled probabilities.","code":""},{"path":[]},{"path":"https://bayesiandemography.github.io/smoothscale/reference/scale_prob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale Probabilities — scale_prob","text":"","code":"## use synthetic census and survey data census <- smoothscale::syn_census survey <- smoothscale::syn_survey  ## calculate national prevalence to use as benchmark prob_national <- sum(survey$total_child_labour) /                    sum(survey$total_all_children)  ## adjust all groups to match benchmark prob_area <- scale_prob(x = census$child_labour,                         size = census$all_children,                         prob_target = prob_national)  ## use tidyverse functions to adjust each ## age-sex group to a different total library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union target <- survey |>   mutate(prob_national = total_child_labour / total_all_children)  census |>   left_join(target, by = c(\"age\", \"sex\")) |>   group_by(age, sex) |>   mutate(prob_area = scale_prob(x = child_labour,                                 size = all_children,                                 prob_target = prob_national)) #> # A tibble: 200 × 9 #> # Groups:   age, sex [4] #>    area    age   sex    child_labour all_children total_child_labour #>    <chr>   <chr> <chr>         <int>        <dbl>              <dbl> #>  1 Area 01 5-9   Female           42          368             222365 #>  2 Area 02 5-9   Female           10           33             222365 #>  3 Area 03 5-9   Female          112          453             222365 #>  4 Area 04 5-9   Female          151          354             222365 #>  5 Area 05 5-9   Female           23          101             222365 #>  6 Area 06 5-9   Female            3            5             222365 #>  7 Area 07 5-9   Female            3           14             222365 #>  8 Area 08 5-9   Female            6           35             222365 #>  9 Area 09 5-9   Female           14           48             222365 #> 10 Area 10 5-9   Female          422         1889             222365 #> # ℹ 190 more rows #> # ℹ 3 more variables: total_all_children <dbl>, prob_national <dbl>, #> #   prob_area <dbl>"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Smooth Probabilities — smooth_prob","title":"Smooth Probabilities — smooth_prob","text":"Calculate probabilities multiple population. Given data number 'trials' 'succession' population, calculate probability success population. instance, given data number respondent, number employed respondents, area, calculate probability employed area. probabilities smoothed: values shifted towards overall mean, values based small sample sizes shifted furtherst.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Smooth Probabilities — smooth_prob","text":"","code":"smooth_prob(x, size, prior_cases = 10)"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Smooth Probabilities — smooth_prob","text":"x Number successes population. numeric vector. size Number trials population. numeric vector. prior_cases Parameter controlling smoothing. Default 10.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Smooth Probabilities — smooth_prob","text":"numeric vector smoothed probabilities.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":"stratifying","dir":"Reference","previous_headings":"","what":"Stratifying","title":"Smooth Probabilities — smooth_prob","text":"often appropriate stratify population smooth separately within strata. instance, estimating probabilities employed, may appropriate divide population strata defined age sex. easiest way stratified smoothing use grouped data frames. See example.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":"prior-counts","dir":"Reference","previous_headings":"","what":"prior_counts","title":"Smooth Probabilities — smooth_prob","text":"argument prior_counts controls degree smoothing. noticeable effect number areas, population per area, small. Larger values prior_counts produce smoothing.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":"mathematical-details","dir":"Reference","previous_headings":"","what":"Mathematical details","title":"Smooth Probabilities — smooth_prob","text":"smoothing based model $$x_k \\sim \\text{Binom}(n_k, \\pi_k)$$ $$\\pi_k \\sim \\text{Beta}(\\lambda \\nu, (1 - \\lambda) \\nu)$$ $$\\lambda \\sim \\text{Unif}(0, 1)$$ $$\\nu \\sim \\text{LogNormal}^+(\\log M, 1)$$ \\(k\\) indexes area population, \\(x_k\\) number successes, specified argument x \\(n_k\\) number trials, specified argument size \\(\\pi_k\\) probability success, \\(M\\) control smoothing, can specified argument prior_counts. smooth_prob() returns \\(\\hat{\\pi}_k\\), maximum posterior density estimate \\(\\pi_k\\). \"direct\" (unsmoothed) estimate probability success \\(x_k / n_k\\). details model, see vignette LINK VIGNETTE.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smooth_prob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Smooth Probabilities — smooth_prob","text":"","code":"## use synthetic census data census <- smoothscale::syn_census  ## smooth all groups towards the national level smoothed <- smooth_prob(x = census$child_labour,                         size = census$all_children) smoothed #>   [1] 0.1205323 0.3007721 0.2486283 0.4217355 0.2356759 0.3776814 0.2540063 #>   [8] 0.2058644 0.2924516 0.2239079 0.2545633 0.1350277 0.2660161 0.2883652 #>  [15] 0.3437973 0.3675160 0.1483832 0.2450890 0.1086436 0.2215408 0.2854412 #>  [22] 0.3709231 0.2163642 0.1233066 0.1982336 0.2856093 0.2248202 0.3380457 #>  [29] 0.2318525 0.1311424 0.3165574 0.1768094 0.2755290 0.3578148 0.2317961 #>  [36] 0.3878768 0.2242419 0.1923907 0.2302324 0.2967493 0.2996808 0.4162655 #>  [43] 0.2406016 0.2450890 0.2121073 0.1626019 0.2694820 0.2510393 0.2432360 #>  [50] 0.2199017 0.1408026 0.3073848 0.2533793 0.4574859 0.2921075 0.2556513 #>  [57] 0.2635971 0.3298322 0.1651697 0.2448263 0.3084629 0.3046090 0.2546887 #>  [64] 0.4533667 0.3293704 0.4941331 0.1241034 0.1748751 0.1068550 0.2660161 #>  [71] 0.2473347 0.4071273 0.2089473 0.1378428 0.2476872 0.2579587 0.2519462 #>  [78] 0.2980855 0.2919887 0.1101094 0.3667327 0.2129209 0.3445193 0.3279501 #>  [85] 0.2258112 0.4045270 0.2161521 0.1724420 0.2668160 0.2101998 0.3169499 #>  [92] 0.3913688 0.1230403 0.3153029 0.2660161 0.2442866 0.2153822 0.3032731 #>  [99] 0.3212769 0.2769412 0.1804302 0.2918149 0.3369369 0.5227290 0.3764610 #> [106] 0.3776814 0.2635971 0.4442049 0.2071769 0.3283814 0.3625966 0.3724415 #> [113] 0.1626019 0.4463724 0.4037881 0.5615725 0.1091029 0.2450890 0.1543658 #> [120] 0.2783979 0.3006354 0.4927858 0.1758988 0.1678941 0.3575088 0.3519603 #> [127] 0.3019739 0.4611818 0.3249433 0.1603462 0.2715621 0.2129209 0.3852343 #> [134] 0.4221915 0.3932817 0.5226584 0.2912068 0.1923907 0.2975963 0.3881023 #> [141] 0.3507243 0.4380781 0.1344564 0.3153029 0.3398517 0.3131803 0.2153822 #> [148] 0.3307152 0.3590365 0.2715621 0.1784770 0.4173126 0.3736966 0.5401010 #> [155] 0.4517846 0.2043282 0.2635971 0.4750223 0.2516317 0.3082693 0.3771943 #> [162] 0.2706927 0.2215408 0.4739919 0.4463724 0.5390927 0.1483832 0.2540063 #> [169] 0.1794613 0.3714888 0.2700076 0.5301347 0.2574875 0.2312844 0.2812681 #> [176] 0.3702633 0.3419224 0.4440680 0.3714888 0.2036228 0.4239965 0.1862431 #> [183] 0.4093397 0.3940101 0.3322635 0.5083383 0.3290645 0.2888125 0.3442008 #> [190] 0.2017042 0.4047612 0.5542312 0.1145517 0.3046090 0.2318525 0.2972702 #> [197] 0.4096207 0.4148709 0.3357356 0.3235478  ## compare smoothed and unsmoothed (\"direct\") estimates unsmoothed <- census$child_labour / census$all_children rbind(head(smoothed), head(unsmoothed)) #>           [,1]      [,2]      [,3]      [,4]      [,5]      [,6] #> [1,] 0.1205323 0.3007721 0.2486283 0.4217355 0.2356759 0.3776814 #> [2,] 0.1141304 0.3030303 0.2472406 0.4265537 0.2277228 0.6000000  ## use tidyverse functions to smooth ## each age-sex group towards a ## different average library(dplyr, warn.conflicts = FALSE) census |>   group_by(age, sex) |>   mutate(smoothed = smooth_prob(x = child_labour,                                 size = all_children)) #> # A tibble: 200 × 6 #> # Groups:   age, sex [4] #>    area    age   sex    child_labour all_children smoothed #>    <chr>   <chr> <chr>         <int>        <dbl>    <dbl> #>  1 Area 01 5-9   Female           42          368    0.121 #>  2 Area 02 5-9   Female           10           33    0.278 #>  3 Area 03 5-9   Female          112          453    0.247 #>  4 Area 04 5-9   Female          151          354    0.416 #>  5 Area 05 5-9   Female           23          101    0.230 #>  6 Area 06 5-9   Female            3            5    0.308 #>  7 Area 07 5-9   Female            3           14    0.231 #>  8 Area 08 5-9   Female            6           35    0.199 #>  9 Area 09 5-9   Female           14           48    0.276 #> 10 Area 10 5-9   Female          422         1889    0.224 #> # ℹ 190 more rows"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smoothscale-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Package 'smoothscale' — smoothscale-package","title":"Package 'smoothscale' — smoothscale-package","text":"smoothscale contains two functions simple small area estimation: smooth_prob() deals sampling error probabilities pulling towards overall average scale_prob() deals measurement error probabilities scaling consistent (reliable) overall total!","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/smoothscale-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Package 'smoothscale' — smoothscale-package","text":"smoothscale also contains synthetic dataset called syn_census.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/syn_census.html","id":null,"dir":"Reference","previous_headings":"","what":"A synthetic census dataset — syn_census","title":"A synthetic census dataset — syn_census","text":"synthetic (ie made-) dataset, illustrating sort tabulations can produced census file. synthetic data measure child labour area level.","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/syn_census.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A synthetic census dataset — syn_census","text":"","code":"syn_census"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/syn_census.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A synthetic census dataset — syn_census","text":"tibble 80 rows following columns: area: Geographical area, numbered 1 20 age: \"5-9\" \"10-14\"` sex: \"Female\" \"Male\" child_labour: Number children involved child labour all_children: Total number children, including involved child labour","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/syn_survey.html","id":null,"dir":"Reference","previous_headings":"","what":"A synthetic survey dataset — syn_survey","title":"A synthetic survey dataset — syn_survey","text":"synthetic (ie made-) dataset, illustrating sort ta bulations can produced survey file. synthetic data measure child labour national level","code":""},{"path":"https://bayesiandemography.github.io/smoothscale/reference/syn_survey.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A synthetic survey dataset — syn_survey","text":"","code":"syn_survey"},{"path":"https://bayesiandemography.github.io/smoothscale/reference/syn_survey.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A synthetic survey dataset — syn_survey","text":"tibble 4 rows following columns: age: \"5-9\" \"10-14\"` sex: \"Female\" \"Male\" total_child_labour: Estimated number children involved child labour whole country total_all_children: Estimated number children, including involved child labour, whole country","code":""}]
